{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a modification of the notebook provided by the MLC group that is available [here](https://mlc.ai/mlc-llm/docs/get_started/try_out.html).\n",
    "\n",
    "An obvious application of LLMs in healthcare is the simplification and or summarization of text. This summarization may be targeted for a clinician to help orient themselves to a complex patient's case or to a patient who is trying to understand her circumstances. It is the later question that we are going to address in this tutorial.\n",
    "\n",
    "Perhaps the most problematic text for a patient to undertsand are nursing notes, because the nursing notes are terse, often oacking context, and filled with cryptic abbreviations and acronyms.\n",
    "\n",
    "This notebook makes use of a SQLite3 notebook (`nursing.sqlite`) that has a single table (`nursing`) with the following columns:\n",
    "\n",
    "- `condition`: the primarcy diagnosis for the patient: one of `brainca`, `sepsis`, or `assult`\n",
    "- `id`: the ID of the patient\n",
    "- `note`: the text of the note (in addition to nursing notes, there might be ntoes from social workers, respiratory therapists, dieticians, etc.)\n",
    "\n",
    "Because of privacy laws, it would be inappropriate to use tools like ChatGPT to process actual clinical texts and large LLMs, like the publicly available 70 Gbyte Llama 2, could not be hosted by small healthcare organizations. What we want to explore here is whether smaller LLMs that might be feasibily hosted by a small healthcare organization, can provide adequate results.\n",
    "\n",
    "We will start with the smaller of the available Llama 2 models (`mlc-chat-Llama-2-7b-chat-hf-q4f16_1`). If you have time, you can try the larger Llama 2 model, but if running on Colab beware that you might run out of GPU credits.\n",
    "\n",
    "In order to save GPU credits, it might be beneficial to initially switch to change \"Runtime type\" to CPU until the models are downloaded.\n",
    "\n",
    "__Note__: The are other language models available from MLC. Feel free to explore different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm85Ap3zDmYB"
   },
   "source": [
    "# Getting Started with MLC-LLM using the Llama 2 Model\n",
    "\n",
    "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ttPt-hNDmYC"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Let's set up your environment, so you can successfully run the `ChatModule`. First, let's set up the Conda environment which we will be running this notebook in (not required if running in Google Colab).\n",
    "\n",
    "```bash\n",
    "conda create --name mlc-llm python=3.10\n",
    "conda activate mlc-llm\n",
    "```\n",
    "\n",
    "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
    "\n",
    "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KK25HZsIDmYC"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWOtpjJMDmYE"
   },
   "source": [
    "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PgW-5OAADmYE",
    "outputId": "e02cc9d4-125b-402e-b0be-0700f25b18f8"
   },
   "outputs": [],
   "source": [
    "#!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels\n",
    "#!pip install --pre --force-reinstall mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn7MYEFt5tvY"
   },
   "source": [
    "**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.kill(os.getpid(), 9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtxnxVIddepN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuQ2vEwAOz2O",
    "outputId": "031607ba-f8ca-4176-d882-2ecfa88cc0d8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhHy_QHF29Si"
   },
   "source": [
    "import sqlite3 as sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwsWd1WbDmYE"
   },
   "source": [
    "Next, let's download the model weights for the Llama2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppvAhErV3gjq"
   },
   "source": [
    "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which model you want to download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0GjINnMDmYF",
    "outputId": "71d6636f-70d2-4908-bbaa-9990ec19700f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git lfs install\n",
    "mkdir -p dist/prebuilt\n",
    "git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\n",
    "cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\n",
    "#cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f16_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYwjsCOK7Jij"
   },
   "source": [
    "These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded.\n",
    "\n",
    "#### Make sure Runtime type is set to GPU now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Ru5__tDmYF"
   },
   "source": [
    "## Let's Chat!\n",
    "\n",
    "Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7b = \"Llama-2-7b-chat-hf-q4f16_1\"\n",
    "model13b = \"Llama-2-13b-chat-hf-q4f16_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you downloaded a different model, change the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJAt6oW7DmYF"
   },
   "outputs": [],
   "source": [
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout\n",
    "\n",
    "cm = ChatModule(model=model7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkV154O33o3Q"
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sq\n",
    "#conn = sq.connect(\"/content/drive/MyDrive/COMP90089/nursing.sqlite\")\n",
    "conn = sq.connect(\"./nursing.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "import markdown\n",
    "import sqlite3 as sq\n",
    "report = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_case(change):\n",
    "    dr.value = markdown.markdown(dd.value)\n",
    "    resp.value = \"\"\n",
    "    \n",
    "def submit_sql(b):\n",
    "    global report\n",
    "    conn = sq.connect(\"nursing.sqlite\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(dd.value)\n",
    "    data = cur.fetchone()\n",
    "    report = data[2]\n",
    "    dr.value = markdown.markdown(data[2])\n",
    "\n",
    "def submit_query(b):\n",
    "    global status\n",
    "    global rsp\n",
    "    global query_history\n",
    "    status.value = \"<h4>Submitted query</h4>\"\n",
    "    q = to.value + report\n",
    "    \n",
    "    \n",
    "    print(\"execute query\")\n",
    "    response = cm.generate(\n",
    "        prompt=q,\n",
    "        progress_callback=StreamToStdout(callback_interval=2),\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(\"update widgets\")\n",
    "    rsp.value = markdown.markdown(response)\n",
    "    status.value = \"<h4>Awaiting query</h4>\"\n",
    "    \n",
    "def submit_conversation(b):\n",
    "    global status\n",
    "    global rsp\n",
    "    global query_history\n",
    "    status.value = \"<h4>Submitted query</h4>\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"execute query\")\n",
    "    response = cm.generate(\n",
    "        prompt=conv.value,\n",
    "        progress_callback=StreamToStdout(callback_interval=2),\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(\"update widgets\")\n",
    "    rsp.value = markdown.markdown(response)\n",
    "    status.value = \"<h4>Awaiting query</h4>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit_sql(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define widgets\n",
    "\n",
    "\n",
    "dd = ipw.Textarea(\"\"\"SELECT * FROM nursing ORDER BY RANDOM() LIMIT 1\"\"\")\n",
    "conv = ipw.Textarea(\"\"\"continue generating\"\"\")\n",
    "\n",
    "dr = ipw.HTML(markdown.markdown(\"Text here\"))\n",
    "qr = ipw.HTML()\n",
    "subcon = ipw.Button(\n",
    "    description = \"Continue chat\",\n",
    "    icon = \"bullseye\")\n",
    "subsql = ipw.Button(\n",
    "    description=\"Run SQL\",\n",
    "    icon=\"bullseye\")\n",
    "submit = ipw.Button(\n",
    "    description='Submit to Chat',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='bullseye' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "to = ipw.Textarea(\n",
    "    value=\"\"\"Identify and enumerate all the anatomic nouns in the following text. For each noun provide a brief definition in layman terms.\n",
    "\n",
    "Text to process: \"\"\",\n",
    "    placeholder='Type something',\n",
    "    description='Prompt:',\n",
    "    layout=ipw.Layout(height=\"auto\", width=\"auto\"),\n",
    "    disabled=False\n",
    ")\n",
    "rsp = ipw.HTML()\n",
    "status = ipw.HTML(\"<h4>Awaiting query</h4>\")\n",
    "\n",
    "\n",
    "# Define Observers\n",
    "\n",
    "submit.on_click(submit_query)\n",
    "subsql.on_click(submit_sql)\n",
    "subcon.on_click(submit_conversation)\n",
    "\n",
    "#dd.observe(update_case, names=\"value\", type=\"change\")\n",
    "\n",
    "# Define Layout\n",
    "\n",
    "grid = ipw.GridspecLayout(5, 2, height=\"512px\")\n",
    "grid[0,:] = status\n",
    "grid[1,0] = subsql\n",
    "grid[1,1] = submit\n",
    "grid[2,0] = dd\n",
    "grid[2,1] = to\n",
    "grid[3,0] = dr\n",
    "grid[3,1] = rsp\n",
    "grid[4,1] = subcon\n",
    "grid[4,2] = conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEaVXnnJDmYF"
   },
   "source": [
    "That is all what needed to set up the `ChatModule`. You can now chat with the model by entering any prompt you'd like. Try it out below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU3DZqaAcQcc"
   },
   "source": [
    "You can also repeat running the code block below for multiple rounds to interact with the model in a chat style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIPVQ8uQcQcd",
    "outputId": "bb95e410-5ed2-40a7-b03b-818d764be5dc"
   },
   "outputs": [],
   "source": [
    "#prompt = input(\"Prompt: \")\n",
    "prompt = to.value + report\n",
    "output = cm.generate(prompt=prompt,  progress_callback=StreamToStdout(callback_interval=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
